# AdaDetectGPT 代码函数中文解释

本文档详细解释了AdaDetectGPT项目中主要代码函数的功能、参数和返回值。

## 1. detect_gpt_ada.py

### 1.1 get_classification_stat
```python
def get_classification_stat(logits_ref, logits_score, labels, w_func, shift_value=None):
```

**功能**：计算分类统计量，用于判断文本是否为LLM生成。

**参数**：
- `logits_ref`：参考模型的输出logits
- `logits_score`：评分模型的输出logits
- `labels`：文本的标签（token IDs）
- `w_func`：见证函数，用于转换对数概率
- `shift_value`：偏移值，用于调整统计量

**返回值**：
- 计算得到的分类统计量（数值）

### 1.2 experiment
```python
def experiment(args):
```

**功能**：主实验函数，执行完整的检测流程。

**参数**：
- `args`：命令行参数，包含各种配置选项

**流程**：
1. 加载模型和分词器
2. 加载数据集
3. 学习见证函数w
4. 计算每个文本的分类统计量
5. 计算评估指标（ROC AUC、PR AUC等）
6. 保存结果

## 2. local_infer_ada.py

### 2.1 load_model_and_tokenizer
```python
def load_model_and_tokenizer(args):
```

**功能**：加载评分模型和采样模型及其分词器。

**参数**：
- `args`：命令行参数

**返回值**：
- `scoring_tokenizer`：评分模型的分词器
- `scoring_model`：评分模型
- `sampling_tokenizer`：采样模型的分词器
- `sampling_model`：采样模型

### 2.2 run
```python
def run(args, scoring_tokenizer, scoring_model, sampling_tokenizer, sampling_model):
```

**功能**：运行推理，检测输入文本是否为LLM生成。

**参数**：
- `args`：命令行参数
- `scoring_tokenizer`：评分模型的分词器
- `scoring_model`：评分模型
- `sampling_tokenizer`：采样模型的分词器
- `sampling_model`：采样模型

**流程**：
1. 初始化见证函数w
2. 如果使用预训练参数，则加载预训练的beta值
3. 否则，从训练数据中学习beta值
4. 对输入文本进行分词和处理
5. 计算分类统计量
6. 返回结果

## 3. nuisance_func.py

### 3.1 BSplineTwoSample类

**功能**：双样本B样条见证函数，用于自适应学习LLM生成文本的检测边界。

#### 3.1.1 __init__
```python
def __init__(self, bspline_args, device):
```
**参数**：
- `bspline_args`：B样条参数，包含start、end、n_bases等
- `device`：计算设备（如"cuda"或"cpu"）

#### 3.1.2 inv_sqrt_matrix
```python
def inv_sqrt_matrix(self, M: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:
```
**功能**：计算矩阵的逆平方根。

#### 3.1.3 solve_beta_star
```python
def solve_beta_star(self, A: torch.Tensor, c: torch.Tensor, d: torch.Tensor) -> torch.Tensor:
```
**功能**：求解优化问题，得到最佳的beta值。

#### 3.1.4 compute_beta_hat
```python
def compute_beta_hat(self, z_u_list, z_v_list, constraint) -> torch.Tensor:
```
**功能**：计算beta_hat值，用于构建见证函数。

**参数**：
- `z_u_list`：人类文本的对数概率列表
- `z_v_list`：LLM文本的对数概率列表
- `constraint`：是否使用约束

#### 3.1.5 get_zij
```python
def get_zij(self, token_list, model, args):
```
**功能**：获取文本中每个token的对数概率。

#### 3.1.6 fit
```python
def fit(self, human_token_list, machine_token_list, model, args, constraint=False):
```
**功能**：拟合见证函数，学习beta_hat值。

#### 3.1.7 forward
```python
def forward(self, input: Tensor):
```
**功能**：前向传播，计算见证函数的输出。

### 3.2 get_ci_list
```python
def get_ci_list(text_list, tokenizer, model, w_fun, args):
```
**功能**：获取文本列表的置信区间。

### 3.3 ShiftLearner类

**功能**：学习偏移值，用于调整统计量。

#### 3.3.1 fit
```python
def fit(self, data, tokenizer, model, w_func, args):
```
**功能**：拟合偏移学习器，计算c_hat值。

## 4. 核心算法流程

### 4.1 见证函数学习流程
1. 从训练数据中获取人类文本和LLM文本
2. 计算每个token的对数概率
3. 使用B样条将对数概率转换为特征
4. 求解优化问题，得到最佳的beta值
5. 使用beta值构建见证函数

### 4.2 文本检测流程
1. 对输入文本进行分词
2. 计算每个token的对数概率
3. 使用见证函数转换对数概率
4. 计算统计量
5. 根据统计量判断文本是否为LLM生成

## 5. 关键概念解释

### 5.1 见证函数（Witness Function）
见证函数w是AdaDetectGPT的核心，用于转换对数概率，以便更好地区分人类文本和LLM文本。

### 5.2 B样条（BSpline）
B样条是一种平滑的分段多项式函数，用于将连续变量（如对数概率）转换为一组特征，便于后续的统计分析。

### 5.3 统计量（Statistic）
统计量是根据对数概率和见证函数计算得到的数值，用于判断文本是否为LLM生成。较大的统计量表示更可能是人类文本，较小的统计量表示更可能是LLM文本。

### 5.4 TNR和FNR
- TNR（True Negative Rate）：真负率，正确识别为人类文本的比例
- FNR（False Negative Rate）：假负率，错误地将LLM文本识别为人类文本的比例

AdaDetectGPT通过最大化TNR的下界来保证检测性能。

## 6. 论文结果复现步骤

以下是复现AdaDetectGPT论文结果的详细步骤：

### 6.1 环境设置

1. 首先确保安装了所需的依赖：
   ```bash
   pip install -r requirements.txt
   # 或使用setup.sh脚本
   bash setup.sh
   ```

2. 确保系统具有CUDA兼容的GPU（推荐）。

### 6.2 快速复现现有实验

项目提供了多个预定义的实验脚本，可以直接运行复现论文中的结果：

#### 6.2.1 白盒实验（Table 1）

```bash
bash exp_whitebox.sh
```

该脚本将：
- 准备数据集（xsum、squad、writing）
- 在5个基础LLM上评估（GPT-2、OPT-2.7B、GPT-Neo-2.7B、GPT-J-6B、GPT-NeoX-20B）
- 比较AdaDetectGPT与Fast-DetectGPT等基线方法

#### 6.2.2 高级白盒实验（Table S7）

```bash
exp_whitebox_advanced.sh
```

评估更先进的开源LLM（Qwen-2.5、Mistral、Llama3）。

#### 6.2.3 黑盒实验（Table 2和Table S8）

```bash
exp_blackbox_advanced.sh
```

评估先进的闭源LLM（Gemini-2.5-Flash、GPT-4o、Claude-3.5-Haiku）。

### 6.3 自定义复现步骤

如果需要自定义实验设置，可以按照以下步骤进行：

#### 6.3.1 准备数据集

使用data_builder.py脚本生成数据集：

```bash
python scripts/data_builder.py \
  --dataset xsum \
  --n_samples 500 \
  --base_model_name gpt2-xl \
  --output_file exp_main/data/xsum_gpt2-xl
```

**参数说明**：
- `--dataset`：数据集名称（xsum、squad、writing等）
- `--n_samples`：样本数量
- `--base_model_name`：用于生成LLM文本的模型名称
- `--output_file`：输出文件路径

#### 6.3.2 运行AdaDetectGPT检测

```bash
python scripts/detect_gpt_ada.py \
  --sampling_model_name gpt2-xl \
  --scoring_model_name gpt2-xl \
  --dataset xsum \
  --train_dataset "./exp_main/data/squad_gpt2-xl&./exp_main/data/writing_gpt2-xl" \
  --dataset_file ./exp_main/data/xsum_gpt2-xl \
  --output_file ./exp_main/results/xsum_gpt2-xl
```

**参数说明**：
- `--sampling_model_name`：采样模型名称
- `--scoring_model_name`：评分模型名称
- `--dataset`：评估数据集名称
- `--train_dataset`：训练数据集（多个数据集用&分隔）
- `--dataset_file`：评估数据集文件路径
- `--output_file`：输出结果文件路径

#### 6.3.3 结果分析

实验结果将保存为JSON文件，包含以下信息：
- 预测结果（real和samples）
- 评估指标（ROC AUC、PR AUC等）
- 计算信息（时间、内存使用等）

可以使用report_results.py脚本汇总和分析结果：

```bash
python scripts/report_results.py --result_files ./exp_main/results/*.json
```

### 6.4 使用预训练模型进行快速推理

如果不想训练模型，可以使用预训练参数进行快速推理：

```bash
python scripts/local_infer_ada.py \
  --text "要检测的文本内容" \
  --w_func pretrained
```

### 6.5 注意事项

1. **内存要求**：实验使用了高内存GPU（96GB），但普通使用场景需要的内存要少得多。

2. **运行时间**：完整复现所有实验可能需要较长时间（数小时到数天），取决于硬件配置。

3. **多GPU支持**：当前实现主要支持单GPU运行。

4. **数据集大小**：论文中的实验使用了500个样本，增加样本数量可以获得更可靠的结果，但会增加运行时间。

通过以上步骤，您可以成功复现AdaDetectGPT论文中的主要结果。